{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('word count').setMaster('local[3]')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = sc.textFile('./data/stopwords.txt')\n",
    "broadcastStopList = sc.broadcast(set(stoplist.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = sc.broadcast(set(\".,:;'!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFileDir = './data'\n",
    "inputFiles = ['4300-0.txt', 'pg19033.txt', 'pg36.txt', 'pg514.txt', 'pg1497.txt', 'pg3207.txt', 'pg42671.txt', 'pg6130.txt']\n",
    "rdds = []\n",
    "N = len(inputFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, inputFile in enumerate(inputFiles):\n",
    "    increment = [0 for _ in range(N)]\n",
    "    increment[i] = 1\n",
    "    lines = sc.textFile(os.path.join(inputFileDir, inputFile))\n",
    "    words = lines.flatMap(lambda line : line.split())\n",
    "    filteredPunctuation = words.map(lambda word : word[1:] if len(word) > 0 and word[0] in punctuations.value else word).map(lambda word : word[:-1] if len(word) > 0 and word[-1] in punctuations.value else word)\n",
    "    filteredWords = filteredPunctuation.filter(lambda word : word.lower() not in broadcastStopList.value)\n",
    "    counts = filteredWords.map(lambda word : (word.lower(), increment)).reduceByKey(lambda a, b : [a[k] + b[k] for k in range(N)])\n",
    "    rdds.append(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unionRdd = rdds[0]\n",
    "for i in range(1, N):\n",
    "    unionRdd = unionRdd.union(rdds[i])\n",
    "matrixRdd = unionRdd.reduceByKey(lambda a, b : [a[k] + b[k] for k in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = matrixRdd.map(lambda x : (x[0], [k * math.log(N / sum(j > 0 for j in x[1])) for k in x[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for i in range(N):\n",
    "    top5 = tfidf.filter(lambda x : x[1][i] > 2).takeOrdered(5, key=lambda x : -x[1][i])\n",
    "    for j in range(5):\n",
    "        output[top5[j][0]] = top5[j][1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sp4.json', 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
