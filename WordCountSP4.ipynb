{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subproject D:\n",
    "\n",
    "* Up until now, you have been computing what is known as term frequency: word counts are measures of frequency of the words (terms). What is more informative is term frequency inverse document frequency, or TF-IDF. This weights the term frequency by the inverse number of documents the word appears in.  \n",
    "* The IDF term looks like this: $log(N / n_t) $, where $N$ is the number of documents (in this case, 8 books), and $n_t$ is the number of documents the specific word $t$ appears in (therefore, this should be between 1 and 8, inclusive).  \n",
    "* You will also have to compute document-specific term frequencies (i.e., word $w_i$ appears $f_{i,j}$ times in document $j$, but $f_{i,k}$ times in document $k$; you’ll need both those counts!). Then, you multiply the IDF term by each document-specific TF term, and that gives you the TF-IDF score for each document.  \n",
    "* In this case, your output should still contain 40 words, but should contain the top-5 words in each document by their TF-IDF rank \n",
    "\n",
    "## Algorithm\n",
    "* We define the TF-IDF for word $w_i$ in document $j$ as  \n",
    "\\begin{equation}\n",
    "TF\\text{-}IDF_{i, j} = f_{i, j} \\cdot log(\\frac {N} {n_j})\n",
    "\\end{equation}\n",
    "where $n_j = 0, 1, 2, ... N$. In the case, $N = 8$.\n",
    "* It is intuitive to create a 2-dimensional array, $A$, to store the frequency for each word in each specific document, where  \n",
    "\\begin{equation}\n",
    "A[i][j] = \\text{ frequency (times of occurrence) for word, } w_i \\text{, in document } j\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('word count').setMaster('local[3]')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = sc.textFile('./data/stopwords.txt')\n",
    "broadcastStopList = sc.broadcast(set(stoplist.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = sc.broadcast(set(\".,:;'!?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read all text through wholeTextFiles() method, which will return a key-value pair, where the key is the path of each file, the value is the content of each file  \n",
    "* Create a map, where the key is the path of the file, and the value is the index, i.e. 0, 1, ... 7\n",
    "* Split each line into a list of words  \n",
    "* Filter out the words which are in the stopwords list  \n",
    "* Map each word, w, to a tuple, (w, [1, 0, 0, ... 0]). For the second document, the tuple would be (w, [0, 1, 0, ... 0]). etc    \n",
    "* Add the tuples with the same key (word)  \n",
    "* Stripping the words which have leading or trailing punctuations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFiles = sc.wholeTextFiles('./data/4300-0.txt,./data/pg*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFiles = textFiles.count()\n",
    "filenames = textFiles.map(lambda textfile : textfile[0]).collect()\n",
    "fileToIdx = {}\n",
    "for i, filename in enumerate(filenames):\n",
    "    increment = [0 for _ in range(numFiles)]\n",
    "    increment[i] = 1\n",
    "    fileToIdx[filename] = increment\n",
    "broadcastFileToIdx = sc.broadcast(fileToIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = textFiles.flatMap(lambda textfile : [(textfile[0], line) for line in textfile[1].splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda pathLine : [(word.casefold(), broadcastFileToIdx.value[pathLine[0]]) for word in pathLine[1].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredPunctuation = words.filter(lambda wordIncre : len(wordIncre[0]) > 1).map(lambda wordIncre : (wordIncre[0][1:], wordIncre[1]) if len(wordIncre[0]) > 0 and wordIncre[0][0] in punctuations.value else wordIncre).map(lambda wordIncre : (wordIncre[0][:-1], wordIncre[1]) if len(wordIncre[0]) > 0 and wordIncre[0][-1] in punctuations.value else wordIncre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredPunctuation = words.map(lambda wordIncre : (wordIncre[0].strip(\".,:;'!?\"), wordIncre[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredWords = filteredPunctuation.filter(lambda wordIncre : wordIncre[0] not in broadcastStopList.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = filteredWords.reduceByKey(lambda a, b : [a[k] + b[k] for k in range(numFiles)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the TF-IDF based on the definition above-mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = counts.map(lambda x : (x[0], [k * np.log(numFiles / sum(j > 0 for j in x[1])) for k in x[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
       " ('gutenberg', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
       " ('ebook', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
       " ('james',\n",
       "  [32.367365349386965,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   3.9233170120469048,\n",
       "   3.9233170120469048,\n",
       "   0.0,\n",
       "   0.0]),\n",
       " ('use', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for i in range(numFiles):\n",
    "    top5 = tfidf.filter(lambda x : x[1][i] > 2).takeOrdered(5, key=lambda x : -x[1][i])\n",
    "    for j in range(5):\n",
    "        output[top5[j][0]] = top5[j][1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stephen': 1000.211381548001,\n",
       " 'bloom': 614.12840197611149,\n",
       " 'j': 365.98171133565108,\n",
       " 'don’t': 361.8228282522914,\n",
       " 'dedalus': 332.71064666877373,\n",
       " 'republic': 228.73856958478194,\n",
       " 'glaucon': 178.83197258446589,\n",
       " 'thrasymachus': 178.83197258446589,\n",
       " 'plato': 151.81117224637262,\n",
       " 'adeimantus': 149.71979100094816,\n",
       " 'alice': 230.12486394590184,\n",
       " '[illustration]': 45.747713916956386,\n",
       " 'rabbit': 29.424877590351784,\n",
       " 'mouse': 23.539902072281429,\n",
       " 'caterpillar': 19.408121055678468,\n",
       " 'martians': 330.63120512709389,\n",
       " 'martian': 106.74466580623158,\n",
       " 'woking': 103.97207708399179,\n",
       " 'pit': 77.485510987926375,\n",
       " 'heat-ray': 70.701012417114413,\n",
       " 'jo': 1651.0765840937897,\n",
       " 'meg': 1276.7771065914192,\n",
       " 'amy': 1139.53396484055,\n",
       " 'laurie': 1089.627367840234,\n",
       " 'beth': 540.65480083675732,\n",
       " 'soveraign': 1023.0852385064792,\n",
       " 'common-wealth': 867.12712288049147,\n",
       " 'onely': 856.72991517209232,\n",
       " 'civill': 675.81850104594662,\n",
       " 'kingdome': 592.64083937875318,\n",
       " 'hector': 500.4522643642805,\n",
       " 'thy': 437.10337519853414,\n",
       " 'achilles': 391.35087195167876,\n",
       " \"o'er\": 356.04101884325661,\n",
       " 'jove': 330.53945826495175,\n",
       " 'bennet': 576.00530704531445,\n",
       " 'bingley': 494.90708691980092,\n",
       " 'darcy': 474.11267150300256,\n",
       " 'elizabeth': 402.02536472476828,\n",
       " 'jane': 350.7324733633323}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sp4.json', 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
