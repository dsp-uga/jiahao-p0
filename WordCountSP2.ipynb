{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subproject B: \n",
    "\n",
    "Implement a list of stopwords (use the stopwords.txt file). These words do not carry much meaning about the specific text. Re-generate the list of top 40 words across all documents, dropping words entirely that are found in the provided list of stopwords. As before, your counts should be case-insensitive, but otherwise no additional preprocessing is needed (HINT: you will need to look at broadcast variables to do this). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisrt, import some Spark classes into the program, and create a SparkContext object, which tells Spark how to access a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('word count').setMaster('local[3]')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD by referencing datasets in an external storage system. We pass all file paths together (with wildcards), as we are going to count the words across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('./data/4300-0.txt,./data/pg*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD for the stopwords, and use the broadcast variable, which is a read-only variable to the cluster. It can be used to give every node to access the same dataset in an efficient manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = sc.textFile('./data/stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastStopList = sc.broadcast(set(stoplist.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'all',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'been',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'could',\n",
       " 'do',\n",
       " 'for',\n",
       " 'from',\n",
       " 'had',\n",
       " 'has',\n",
       " 'he',\n",
       " 'her',\n",
       " 'him',\n",
       " 'his',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'may',\n",
       " 'me',\n",
       " 'my',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'said',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'they',\n",
       " 'this',\n",
       " 'to',\n",
       " 'very',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'which',\n",
       " 'who',\n",
       " 'will',\n",
       " 'with',\n",
       " 'would',\n",
       " 'your'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastStopList.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split each line into a list of words  \n",
    "Filter out the words which are in the stopwords list  \n",
    "Map each word, w, to a tuple, (w, 1)  \n",
    "Add the tuples with the same key (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line : line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredWords = words.filter(lambda word : word.lower() not in broadcastStopList.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = filteredWords.map(lambda word : (word.lower(), 1)).reduceByKey(lambda a, b : a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top 40 words (drop words with counts less than 2 can improve the performance a littlt bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = counts.filter(lambda x : x[1] > 2).takeOrdered(40, key=lambda x : -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 11044),\n",
       " ('not', 8141),\n",
       " ('you', 6354),\n",
       " ('have', 5146),\n",
       " ('no', 3620),\n",
       " ('one', 3498),\n",
       " ('like', 2253),\n",
       " ('more', 2087),\n",
       " ('out', 2021),\n",
       " ('up', 1831),\n",
       " ('man', 1783),\n",
       " ('now', 1579),\n",
       " ('only', 1555),\n",
       " ('must', 1523),\n",
       " ('little', 1485),\n",
       " ('those', 1447),\n",
       " ('good', 1444),\n",
       " ('should', 1417),\n",
       " ('after', 1379),\n",
       " ('great', 1358),\n",
       " ('every', 1356),\n",
       " ('first', 1318),\n",
       " ('own', 1289),\n",
       " ('did', 1271),\n",
       " ('how', 1266),\n",
       " ('see', 1251),\n",
       " ('these', 1244),\n",
       " ('men', 1233),\n",
       " ('over', 1209),\n",
       " ('where', 1205),\n",
       " ('make', 1196),\n",
       " ('upon', 1188),\n",
       " ('nor', 1181),\n",
       " ('never', 1177),\n",
       " ('much', 1167),\n",
       " ('time', 1166),\n",
       " ('said,', 1163),\n",
       " ('two', 1142),\n",
       " ('old', 1140),\n",
       " ('made', 1128)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the list to a dictionary, and output to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dict(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sp2.json', 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
